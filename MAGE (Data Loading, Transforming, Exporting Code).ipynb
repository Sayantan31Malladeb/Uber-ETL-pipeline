{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af99f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "if 'data_loader' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_loader\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "@data_loader\n",
    "def load_data_from_api(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Template for loading data from API\n",
    "    \"\"\"\n",
    "    url = 'https://storage.googleapis.com/uber_datapipeline_sayantan/uber_data.csv'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return pd.read_csv(io.StringIO(response.text), sep=',')\n",
    "\n",
    "\n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if 'transformer' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import transformer\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "@transformer\n",
    "def transform(df, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Template code for a transformer block.\n",
    "\n",
    "    Add more parameters to this function if this block has multiple parent blocks.\n",
    "    There should be one parameter for each output variable from each parent block.\n",
    "\n",
    "    Args:\n",
    "        data: The output from the upstream parent block\n",
    "        args: The output from any additional upstream blocks (if applicable)\n",
    "\n",
    "    Returns:\n",
    "        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n",
    "    \"\"\"\n",
    "    # Specify your transformation logic here\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "    # Selecting the datetime columns and resetting the index\n",
    "    datetime_dim = df[['tpep_pickup_datetime','tpep_dropoff_datetime']].reset_index(drop=True)\n",
    "\n",
    "    # Removing Duplicates\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    df['trip_id'] = df.index\n",
    "\n",
    "    # Extracting pickup datetime components\n",
    "    datetime_dim['pickup_hour'] = datetime_dim['tpep_pickup_datetime'].dt.hour\n",
    "    datetime_dim['pickup_day'] = datetime_dim['tpep_pickup_datetime'].dt.day\n",
    "    datetime_dim['pickup_month'] = datetime_dim['tpep_pickup_datetime'].dt.month\n",
    "    datetime_dim['pickup_year'] = datetime_dim['tpep_pickup_datetime'].dt.year\n",
    "    datetime_dim['pickup_weekday'] = datetime_dim['tpep_pickup_datetime'].dt.weekday\n",
    "\n",
    "    # Extracting dropoff datetime components\n",
    "    datetime_dim['drop_hour'] = datetime_dim['tpep_dropoff_datetime'].dt.hour\n",
    "    datetime_dim['drop_day'] = datetime_dim['tpep_dropoff_datetime'].dt.day\n",
    "    datetime_dim['drop_month'] = datetime_dim['tpep_dropoff_datetime'].dt.month\n",
    "    datetime_dim['drop_year'] = datetime_dim['tpep_dropoff_datetime'].dt.year\n",
    "    datetime_dim['drop_weekday'] = datetime_dim['tpep_dropoff_datetime'].dt.weekday\n",
    "\n",
    "    # Adding a unique datetime_id for each row\n",
    "    datetime_dim['datetime_id'] = datetime_dim.index\n",
    "\n",
    "    # Reordering the columns\n",
    "    datetime_dim = datetime_dim[['datetime_id', 'tpep_pickup_datetime', 'pickup_hour', 'pickup_day', 'pickup_month', 'pickup_year', 'pickup_weekday',\n",
    "                             'tpep_dropoff_datetime', 'drop_hour', 'drop_day', 'drop_month', 'drop_year', 'drop_weekday']]\n",
    "\n",
    "    # Create passenger_count_dim DataFrame with passenger_count and a unique passenger_count_id\n",
    "    passenger_count_dim = df[['passenger_count']].reset_index(drop=True)\n",
    "    passenger_count_dim['passenger_count_id'] = passenger_count_dim.index\n",
    "    passenger_count_dim = passenger_count_dim[['passenger_count_id', 'passenger_count']]\n",
    "\n",
    "    # Create trip_distance_dim DataFrame with trip_distance and a unique trip_distance_id\n",
    "    trip_distance_dim = df[['trip_distance']].reset_index(drop=True)\n",
    "    trip_distance_dim['trip_distance_id'] = trip_distance_dim.index\n",
    "    trip_distance_dim = trip_distance_dim[['trip_distance_id', 'trip_distance']]\n",
    "\n",
    "    # Define a dictionary for rate code types\n",
    "    rate_code_type = {\n",
    "        1: \"Standard rate\",\n",
    "        2: \"JFK\",\n",
    "        3: \"Newark\",\n",
    "        4: \"Nassau or Westchester\",\n",
    "        5: \"Negotiated fare\",\n",
    "        6: \"Group ride\"\n",
    "    }\n",
    "\n",
    "    # Create rate_code_dim DataFrame with RatecodeID, rate_code_id, and rate_code_name\n",
    "    rate_code_dim = df[['RatecodeID']].reset_index(drop=True)\n",
    "    rate_code_dim['rate_code_id'] = rate_code_dim.index\n",
    "    rate_code_dim['rate_code_name'] = rate_code_dim['RatecodeID'].map(rate_code_type)\n",
    "    rate_code_dim = rate_code_dim[['rate_code_id', 'RatecodeID', 'rate_code_name']]\n",
    "\n",
    "    # Create a DataFrame for pickup location dimensions, including latitude and longitude.\n",
    "    pickup_location_dim = df[['pickup_longitude', 'pickup_latitude']].reset_index(drop=True)\n",
    "\n",
    "    # Add a column 'pickup_location_id' with unique index values.\n",
    "    pickup_location_dim['pickup_location_id'] = pickup_location_dim.index\n",
    "\n",
    "    # Rearrange the columns to place 'pickup_location_id' first.\n",
    "    pickup_location_dim = pickup_location_dim[['pickup_location_id', 'pickup_latitude', 'pickup_longitude']]\n",
    "\n",
    "    # Create a DataFrame for dropoff location dimensions, including latitude and longitude.\n",
    "    dropoff_location_dim = df[['dropoff_longitude', 'dropoff_latitude']].reset_index(drop=True)\n",
    "\n",
    "    # Add a column 'dropoff_location_id' with unique index values.\n",
    "    dropoff_location_dim['dropoff_location_id'] = dropoff_location_dim.index\n",
    "\n",
    "    # Rearrange the columns to place 'dropoff_location_id' first.\n",
    "    dropoff_location_dim = dropoff_location_dim[['dropoff_location_id', 'dropoff_latitude', 'dropoff_longitude']]\n",
    "\n",
    "    # Define a mapping of payment type codes to their corresponding names.\n",
    "    payment_type_name = {\n",
    "        1: \"Credit card\",\n",
    "        2: \"Cash\",\n",
    "        3: \"No charge\",\n",
    "        4: \"Dispute\",\n",
    "        5: \"Unknown\",\n",
    "        6: \"Voided trip\"\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame containing the 'payment_type' column from the original DataFrame.\n",
    "    payment_type_dim = df[['payment_type']].reset_index(drop=True)\n",
    "\n",
    "    # Add a new column 'payment_type_id' with unique index values.\n",
    "    payment_type_dim['payment_type_id'] = payment_type_dim.index\n",
    "\n",
    "    # Map the 'payment_type' values to their corresponding names using the defined mapping.\n",
    "    payment_type_dim['payment_type_name'] = payment_type_dim['payment_type'].map(payment_type_name)\n",
    "\n",
    "    # Rearrange the columns to place 'payment_type_id' first, followed by 'payment_type' and 'payment_type_name'.\n",
    "    payment_type_dim = payment_type_dim[['payment_type_id', 'payment_type', 'payment_type_name']]\n",
    "\n",
    "    # Merge DataFrames to create a fact table by joining various dimension tables based on common keys ('trip_id').\n",
    "\n",
    "    # Merge with 'passenger_count_dim' using 'trip_id' and 'passenger_count_id' as keys.\n",
    "    # Repeat this process for other dimension tables as well.\n",
    "    fact_table = df.merge(passenger_count_dim, left_on='trip_id', right_on='passenger_count_id') \\\n",
    "             .merge(trip_distance_dim, left_on='trip_id', right_on='trip_distance_id') \\\n",
    "             .merge(rate_code_dim, left_on='trip_id', right_on='rate_code_id') \\\n",
    "             .merge(pickup_location_dim, left_on='trip_id', right_on='pickup_location_id') \\\n",
    "             .merge(dropoff_location_dim, left_on='trip_id', right_on='dropoff_location_id') \\\n",
    "             .merge(datetime_dim, left_on='trip_id', right_on='datetime_id') \\\n",
    "             .merge(payment_type_dim, left_on='trip_id', right_on='payment_type_id') \\\n",
    "             [['trip_id', 'VendorID', 'datetime_id', 'passenger_count_id',\n",
    "               'trip_distance_id', 'rate_code_id', 'store_and_fwd_flag', 'pickup_location_id', 'dropoff_location_id',\n",
    "               'payment_type_id', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "               'improvement_surcharge', 'total_amount']]\n",
    "\n",
    "    return {\"datetime_dim\":datetime_dim.to_dict(orient=\"dict\"),\n",
    "    \"passenger_count_dim\":passenger_count_dim.to_dict(orient=\"dict\"),\n",
    "    \"trip_distance_dim\":trip_distance_dim.to_dict(orient=\"dict\"),\n",
    "    \"rate_code_dim\":rate_code_dim.to_dict(orient=\"dict\"),\n",
    "    \"pickup_location_dim\":pickup_location_dim.to_dict(orient=\"dict\"),\n",
    "    \"dropoff_location_dim\":dropoff_location_dim.to_dict(orient=\"dict\"),\n",
    "    \"payment_type_dim\":payment_type_dim.to_dict(orient=\"dict\"),\n",
    "    \"fact_table\":fact_table.to_dict(orient=\"dict\")}\n",
    "    \n",
    "    return \"success\"\n",
    "\n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d30c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exporting\n",
    "\n",
    "from mage_ai.settings.repo import get_repo_path\n",
    "from mage_ai.io.bigquery import BigQuery\n",
    "from mage_ai.io.config import ConfigFileLoader\n",
    "from pandas import DataFrame\n",
    "from os import path\n",
    "\n",
    "if 'data_exporter' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_exporter\n",
    "\n",
    "@data_exporter\n",
    "def export_data_to_big_query(data, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Template for exporting data to a BigQuery warehouse.\n",
    "    Specify your configuration settings in 'io_config.yaml'.\n",
    "\n",
    "    Docs: https://docs.mage.ai/design/data-loading#bigquery\n",
    "    \"\"\"\n",
    "\n",
    "    config_path = path.join(get_repo_path(), 'io_config.yaml')\n",
    "    config_profile = 'default'\n",
    "\n",
    "    for key, value in data.items():\n",
    "        table_id = 'uber-pipeline-project.uber_data_pipeline.{}'.format(key)\n",
    "        BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n",
    "            DataFrame(value),\n",
    "            table_id,\n",
    "            if_exists='replace',  # Specify resolution policy if table name already exists\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
